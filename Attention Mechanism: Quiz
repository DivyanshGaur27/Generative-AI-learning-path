What is the purpose of the attention weights?
=>To assign weights to different parts of the input sequence, with the most important parts receiving the highest weights.

What is the advantage of using the attention mechanism over a traditional sequence-to-sequence model?
=>The attention mechanism lets the model focus on specific parts of the input sequence.

What are the two main steps of the attention mechanism?
=>Calculating the attention weights and generating the context vector

How does an attention model differ from a traditional model?
=>Attention models pass a lot more information to the decoder.

What is the advantage of using the attention mechanism over a traditional recurrent neural network (RNN) encoder-decoder?
=>The attention mechanism lets the decoder focus on specific parts of the input sequence, which can improve the accuracy of the translation.

What is the name of the machine learning technique that allows a neural network to focus on specific parts of an input sequence?
=>Attention mechanism

What is the name of the machine learning architecture that can be used to translate text from one language to another?
=>Encoder-decoder
